{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of Pygame Learning Environments\n",
    "\n",
    "Objectives:\n",
    "* Learn action space\n",
    "* See visualization\n",
    "* Provide testbed for trying different arguments and game specific hyper paramters\n",
    "\n",
    "Games Tested:\n",
    "* Catcher [1.0]\n",
    "* PixelCopter [1.1]\n",
    "* Pong [1.2]\n",
    "* PuckWorld [1.3]\n",
    "* RaycastMaze [1.5]\n",
    "* Snake [1.6]\n",
    "* Waterworld [1.7]\n",
    "* MonsterKong [2.0]\n",
    "* FlappyBird [2.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catcher [1.0]\n",
    "\n",
    "* Catch falling blocks from the ceiling by touching with paddle\n",
    "* at env start/reset, need to take an action/advance frames or screen will be blank\n",
    "* three actions: left, right, None\n",
    "* seems to be momentum from prior actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 97, None]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ple import PLE\n",
    "from ple.games.catcher import Catcher\n",
    "\n",
    "game = Catcher()\n",
    "\n",
    "fps = 30  # fps we want to run at\n",
    "frame_skip = 2\n",
    "num_steps = 2\n",
    "force_fps = False # False for slower speed\n",
    "display_screen = True\n",
    "\n",
    "# make a PLE instance.\n",
    "ple_env = PLE(game, fps=fps, frame_skip=frame_skip, num_steps=num_steps,\n",
    "        force_fps=force_fps, display_screen=display_screen)\n",
    "\n",
    "action_length = len(ple_env.getActionSet())\n",
    "action_list = ple_env.getActionSet()\n",
    "print(action_list)\n",
    "\n",
    "# init agent and game.\n",
    "ple_env.init()\n",
    "#advance game one frame, starts out at a black screen\n",
    "ple_env.act(ple_env.NOOP)\n",
    "\n",
    "for i in range(100):\n",
    "    action = action_list[np.random.randint(0,action_length)]\n",
    "    ple_env.act(action)\n",
    "    if ple_env.game_over():\n",
    "        ple_env.reset_game()\n",
    "        ple_env.act(ple_env.NOOP)\n",
    "    #ple_env.saveScreen(\"test_screen_capture_{}.png\".format(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#useful utilities\n",
    "#ple_env.act() #like gym.step() except only returns reward \n",
    "#ple_env.getScreenGrayscale() #get grayscale image\n",
    "#ple_env.getScreenRGB() #get RGB image\n",
    "#ple_env.lives() check for remaining lives, can be used to check if reset for env is needed\n",
    "#ple_env.reset_game() #resets the environment\n",
    "#ple_env.NOOP #no action, advances the frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1.1] Pixelcopter\n",
    "\n",
    "* Flappy bird with pixels\n",
    "* Flap or don't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[119, None]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ple import PLE\n",
    "from ple.games.pixelcopter import Pixelcopter\n",
    "\n",
    "game = Pixelcopter()\n",
    "\n",
    "fps = 30  # fps we want to run at\n",
    "frame_skip = 2\n",
    "num_steps = 2\n",
    "force_fps = False # False for slower speed\n",
    "display_screen = True\n",
    "\n",
    "# make a PLE instance.\n",
    "ple_env = PLE(game, fps=fps, frame_skip=frame_skip, num_steps=num_steps,\n",
    "        force_fps=force_fps, display_screen=display_screen)\n",
    "\n",
    "action_length = len(ple_env.getActionSet())\n",
    "action_list = ple_env.getActionSet()\n",
    "print(action_list)\n",
    "\n",
    "# init agent and game.\n",
    "ple_env.init()\n",
    "#advance game one frame, starts out at a black screen\n",
    "ple_env.act(ple_env.NOOP)\n",
    "\n",
    "for i in range(100):\n",
    "    action = action_list[np.random.randint(0,action_length)]\n",
    "    ple_env.act(action)\n",
    "    if ple_env.game_over():\n",
    "        ple_env.reset_game()\n",
    "        ple_env.act(ple_env.NOOP)\n",
    "    #ple_env.saveScreen(\"test_screen_capture_{}.png\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1.2] Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "couldn't import doomish\n",
      "Couldn't import doom\n",
      "[115, 119, None]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ple import PLE\n",
    "from ple.games.pong import Pong\n",
    "\n",
    "game = Pong() # create our game \n",
    "\n",
    "fps = 30  # fps we want to run at\n",
    "frame_skip = 2\n",
    "num_steps = 2\n",
    "force_fps = False # False for slower speed\n",
    "display_screen = True\n",
    "\n",
    "# make a PLE instance.\n",
    "ple_env = PLE(game, fps=fps, frame_skip=frame_skip, num_steps=num_steps,\n",
    "        force_fps=force_fps, display_screen=display_screen)\n",
    "\n",
    "action_length = len(ple_env.getActionSet())\n",
    "action_list = ple_env.getActionSet()\n",
    "print(action_list)\n",
    "\n",
    "# init agent and game.\n",
    "ple_env.init()\n",
    "#advance game one frame, starts out at a black screen\n",
    "ple_env.act(ple_env.NOOP)\n",
    "\n",
    "lives_check = ple_env.lives() \n",
    "\n",
    "for i in range(100):\n",
    "    action = action_list[np.random.randint(0,action_length)]\n",
    "    ple_env.act(action)\n",
    "    if lives_check != ple_env.lives():\n",
    "        print('lives are different {}'.format(ple_env.lives()))\n",
    "    if ple_env.game_over():\n",
    "        ple_env.reset_game()\n",
    "        ple_env.act(ple_env.NOOP)\n",
    "    #ple_env.saveScreen(\"test_screen_capture_{}.png\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1.3] PuckWorld\n",
    "* grab green pucks, avoid red one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[115, 100, 119, 97, None]\n",
      "-1\n",
      "-217.97264343208525\n",
      "-220.0602168950873\n",
      "-218.15165301165268\n",
      "-213.36342334083918\n",
      "-209.31980018582453\n",
      "-208.0605593907046\n",
      "-214.3474880332517\n",
      "-228.43290350606503\n",
      "-232.5967320946138\n",
      "-231.46370647515005\n",
      "-219.6697275516367\n",
      "-203.65271571529638\n",
      "-195.28279019216643\n",
      "-198.26504472941042\n",
      "-207.34650512284452\n",
      "-211.0524507763712\n",
      "-207.58031342291952\n",
      "-204.50276834203714\n",
      "-201.77303846894392\n",
      "-202.80539692156253\n",
      "-208.552734678241\n",
      "-214.18577445935625\n",
      "-215.70027239217052\n",
      "-209.28248360425005\n",
      "-205.1296680331261\n",
      "-215.48985083616753\n",
      "-232.30200438627526\n",
      "-231.31092545303545\n",
      "-229.25745061658472\n",
      "-229.8109915793475\n",
      "-231.93645533269046\n",
      "-229.1695544933964\n",
      "-225.3163804327105\n",
      "-226.88681585564518\n",
      "-224.86797016244418\n",
      "-222.82328490646705\n",
      "-227.70791324040238\n",
      "-229.52636915952462\n",
      "-226.97006044567934\n",
      "-220.784870777361\n",
      "-215.965096024147\n",
      "-219.39960413905646\n",
      "-227.27823325294048\n",
      "-234.9307372972471\n",
      "-233.7139463062049\n",
      "-220.72259536729507\n",
      "-207.81695428722378\n",
      "-193.33941617640266\n",
      "-173.8586303952161\n",
      "-158.71452400697308\n",
      "-145.29031099905114\n",
      "-136.346238236305\n",
      "-138.78264748385118\n",
      "-147.42913049274102\n",
      "-153.1846058954161\n",
      "-153.9891165154204\n",
      "-154.15170790835327\n",
      "-159.74928110625842\n",
      "-176.72020175555735\n",
      "-200.7033636774304\n",
      "-219.3888465193504\n",
      "-216.75354612535557\n",
      "-208.76376989924756\n",
      "-196.43386402034412\n",
      "-181.270064104\n",
      "-167.727518485\n",
      "-161.838426533\n",
      "-162.242885863\n",
      "-169.080537699\n",
      "-170.949536032\n",
      "-162.679411055\n",
      "-148.435049863\n",
      "-134.491692202\n",
      "-120.142348024\n",
      "-107.49502818\n",
      "-97.8262528775\n",
      "-90.1467747884\n",
      "-83.3322830123\n",
      "-71.8189310796\n",
      "-59.5592103651\n",
      "-55.0795920167\n",
      "-54.5802479229\n",
      "-66.9471526737\n",
      "-94.3343817188\n",
      "-130.460045733\n",
      "-175.306120788\n",
      "-225.999375993\n",
      "-243.437862764\n",
      "-246.21241221\n",
      "-248.529989415\n",
      "-260.053299964\n",
      "-260.818137126\n",
      "-251.934763445\n",
      "-240.389642827\n",
      "-229.82986649\n",
      "-221.774338065\n",
      "-212.031296546\n",
      "-211.476575633\n",
      "-212.787822768\n",
      "-208.567060063\n",
      "-200.56985299\n",
      "-189.872732286\n",
      "-185.600829599\n",
      "-189.110952205\n",
      "-189.38451682\n",
      "-188.957955222\n",
      "-184.65529877\n",
      "-179.416018364\n",
      "-183.649796632\n",
      "-185.169191125\n",
      "-179.046348228\n",
      "-175.973263806\n",
      "-179.413514069\n",
      "-186.1658849\n",
      "-195.046895794\n",
      "-203.73875445\n",
      "-212.940687337\n",
      "-227.030255616\n",
      "-236.159228949\n",
      "-246.890309238\n",
      "-248.293818312\n",
      "-247.956545689\n",
      "-247.043587094\n",
      "-251.378408069\n",
      "-107.065660188\n",
      "-106.702224175\n",
      "-104.94291261\n",
      "-100.482573662\n",
      "-88.7958885042\n",
      "-81.9990729305\n",
      "-89.772888957\n",
      "-100.064702507\n",
      "-109.283315711\n",
      "-124.271023115\n",
      "-142.078237444\n",
      "-163.03462755\n",
      "-188.942940594\n",
      "-205.22718641\n",
      "-200.626971456\n",
      "-190.400927207\n",
      "-181.037137026\n",
      "-172.778300456\n",
      "-165.50570742\n",
      "-159.110859018\n",
      "-157.46798471\n",
      "-161.452304798\n",
      "-165.416829645\n",
      "-169.05518262\n",
      "-172.385680198\n",
      "-172.304158437\n",
      "-163.964017973\n",
      "-153.173866631\n",
      "-150.217709243\n",
      "-149.583616148\n",
      "-141.513304819\n",
      "-133.552710718\n",
      "-128.101929604\n",
      "-120.757054147\n",
      "-114.828992326\n",
      "-105.742675338\n",
      "-102.286591504\n",
      "-104.53886218\n",
      "-108.150721286\n",
      "-113.954203036\n",
      "-119.539766819\n",
      "-124.646154006\n",
      "-131.218160615\n",
      "-147.020743021\n",
      "-175.911755955\n",
      "-198.478707252\n",
      "-206.320591375\n",
      "-203.086556513\n",
      "-186.296454761\n",
      "-162.728934114\n",
      "-136.818602231\n",
      "-113.868998755\n",
      "-92.7071467246\n",
      "-73.3957526257\n",
      "-60.9384721467\n",
      "-76.5454350667\n",
      "-103.705890187\n",
      "-105.637796126\n",
      "-107.167104769\n",
      "-103.270476918\n",
      "-93.906552192\n",
      "-85.8538475357\n",
      "-82.9130857151\n",
      "-85.7354910839\n",
      "-84.5292015078\n",
      "-78.1790767381\n",
      "-68.5638264842\n",
      "-55.3132075725\n",
      "-47.4027774884\n",
      "-41.3572946563\n",
      "-28.0372485525\n",
      "-14.397223479\n",
      "-10.7620393351\n",
      "-24.978333552\n",
      "-41.5800320277\n",
      "-64.8722918198\n",
      "-90.6809716124\n",
      "-110.978272597\n",
      "-133.022941444\n",
      "-137.819301918\n",
      "-129.226516441\n",
      "-109.647511822\n",
      "-89.8989231331\n",
      "-80.1657203829\n",
      "-76.4014220808\n",
      "-82.9665463163\n",
      "-97.1728965177\n",
      "-112.407074442\n",
      "-132.875719014\n",
      "-158.825304855\n",
      "-181.567440783\n",
      "-191.202582765\n",
      "-194.701094461\n",
      "-185.876625083\n",
      "-163.757136806\n",
      "-135.175836965\n",
      "-100.939735177\n",
      "-70.117274344\n",
      "-67.7265711905\n",
      "-76.8977921571\n",
      "-84.1300635512\n",
      "-92.8280387037\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-88498a383e71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mple_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mple_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jim/gym_attempts/PyGame-Learning-Environment/ple/ple.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \"\"\"\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_oneStepAct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_draw_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jim/gym_attempts/PyGame-Learning-Environment/ple/ple.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \"\"\"\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_oneStepAct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_draw_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jim/gym_attempts/PyGame-Learning-Environment/ple/ple.py\u001b[0m in \u001b[0;36m_oneStepAct\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0mtime_elapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_elapsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jim/gym_attempts/PyGame-Learning-Environment/ple/ple.py\u001b[0m in \u001b[0;36m_tick\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;36m1000.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jim/gym_attempts/PyGame-Learning-Environment/ple/games/base/pygamewrapper.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, fps)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mThis\u001b[0m \u001b[0msleeps\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgame\u001b[0m \u001b[0mto\u001b[0m \u001b[0mensure\u001b[0m \u001b[0mit\u001b[0m \u001b[0mruns\u001b[0m \u001b[0mat\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdesired\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick_busy_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madjustRewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ple import PLE\n",
    "from ple.games.puckworld import PuckWorld\n",
    "\n",
    "game = PuckWorld() # create our game \n",
    "\n",
    "fps = 30  # fps we want to run at\n",
    "frame_skip = 2\n",
    "num_steps = 2\n",
    "force_fps = False # False for slower speed\n",
    "display_screen = True\n",
    "\n",
    "# make a PLE instance.\n",
    "ple_env = PLE(game, fps=fps, frame_skip=frame_skip, num_steps=num_steps,\n",
    "        force_fps=force_fps, display_screen=display_screen)\n",
    "\n",
    "action_length = len(ple_env.getActionSet())\n",
    "action_list = ple_env.getActionSet()\n",
    "print(action_list)\n",
    "\n",
    "# init agent and game.\n",
    "ple_env.init()\n",
    "#advance game one frame, starts out at a black screen\n",
    "ple_env.act(ple_env.NOOP)\n",
    "print(ple_env.lives())\n",
    "for i in range(1000):\n",
    "    action = action_list[np.random.randint(0,action_length)]\n",
    "    reward = ple_env.act(action)\n",
    "    print(reward)\n",
    "    if ple_env.game_over():\n",
    "        print('game_over')\n",
    "        ple_env.reset_game()\n",
    "        ple_env.act(ple_env.NOOP)\n",
    "    #ple_env.saveScreen(\"test_screen_capture_{}.png\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1.4] Raycast\n",
    "* Navitage a maze from first person view\n",
    "* Search for an exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 119, 97, 115, None]\n",
      "0\n",
      "5.0\n",
      "game over\n",
      "5.0\n",
      "game over\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-78b5eef4f20a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mple_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jim/gym_attempts/PyGame-Learning-Environment/ple/ple.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \"\"\"\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_oneStepAct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_draw_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jim/gym_attempts/PyGame-Learning-Environment/ple/ple.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \"\"\"\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_oneStepAct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_draw_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jim/gym_attempts/PyGame-Learning-Environment/ple/ple.py\u001b[0m in \u001b[0;36m_oneStepAct\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0mtime_elapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_elapsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jim/gym_attempts/PyGame-Learning-Environment/ple/ple.py\u001b[0m in \u001b[0;36m_tick\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;36m1000.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jim/gym_attempts/PyGame-Learning-Environment/ple/games/base/pygamewrapper.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, fps)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mThis\u001b[0m \u001b[0msleeps\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgame\u001b[0m \u001b[0mto\u001b[0m \u001b[0mensure\u001b[0m \u001b[0mit\u001b[0m \u001b[0mruns\u001b[0m \u001b[0mat\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdesired\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick_busy_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madjustRewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ple import PLE\n",
    "from ple.games.raycastmaze import RaycastMaze\n",
    "\n",
    "game = RaycastMaze() # create our game \n",
    "\n",
    "fps = 30  # fps we want to run at\n",
    "frame_skip = 2\n",
    "num_steps = 2\n",
    "force_fps = False # False for slower speed\n",
    "display_screen = True\n",
    "\n",
    "# make a PLE instance.\n",
    "ple_env = PLE(game, fps=fps, frame_skip=frame_skip, num_steps=num_steps,\n",
    "        force_fps=force_fps, display_screen=display_screen)\n",
    "\n",
    "action_length = len(ple_env.getActionSet())\n",
    "action_list = ple_env.getActionSet()\n",
    "print(action_list)\n",
    "\n",
    "# init agent and game.\n",
    "ple_env.init()\n",
    "#advance game one frame, starts out at a black screen\n",
    "ple_env.act(ple_env.NOOP)\n",
    "print(ple_env.lives())\n",
    "\n",
    "for i in range(10000):\n",
    "    action = action_list[np.random.randint(0,action_length)]\n",
    "    reward = ple_env.act(action)\n",
    "    if reward != 0:\n",
    "        print(reward)\n",
    "    if ple_env.game_over():\n",
    "        print('game over')\n",
    "        ple_env.reset_game()\n",
    "        ple_env.act(ple_env.NOOP)\n",
    "    #ple_env.saveScreen(\"test_screen_capture_{}.png\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1.5] Snake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[115, 100, 119, 97, None]\n",
      "[97, 100, 115, 119, None]\n",
      "[97, 100, 115, 119, None]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ple import PLE\n",
    "from ple.games.snake import Snake\n",
    "\n",
    "game = Snake() # create our game\n",
    "\n",
    "fps = 30  # fps we want to run at\n",
    "frame_skip = 2\n",
    "num_steps = 2\n",
    "force_fps = False # False for slower speed\n",
    "display_screen = True\n",
    "\n",
    "# make a PLE instance.\n",
    "ple_env = PLE(game, fps=fps, frame_skip=frame_skip, num_steps=num_steps,\n",
    "        force_fps=force_fps, display_screen=display_screen)\n",
    "\n",
    "\n",
    "\n",
    "# init agent and game.\n",
    "ple_env.init()\n",
    "\n",
    "action_length = len(ple_env.getActionSet())\n",
    "action_list = ple_env.getActionSet()\n",
    "print(action_list)\n",
    "\n",
    "# >>> l = [1, 3, 2, 5, 4, None, 7]\n",
    "# >>> sorted(l, key=lambda x: (x is None, x))\n",
    "# [1, 2, 3, 4, 5, 7, None]\n",
    "\n",
    "print(sorted(action_list, key=lambda x: (x is None, x)))\n",
    "action_list = sorted(action_list, key=lambda x: (x is None, x))\n",
    "print(action_list)\n",
    "\n",
    "#advance game one frame, starts out at a black screen\n",
    "ple_env.act(ple_env.NOOP)\n",
    "\n",
    "for i in range(1):\n",
    "    action = action_list[np.random.randint(0,action_length)]\n",
    "    ple_env.act(action)\n",
    "    if ple_env.game_over():\n",
    "        ple_env.reset_game()\n",
    "        ple_env.act(ple_env.NOOP)\n",
    "    #ple_env.saveScreen(\"test_screen_capture_{}.png\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1.6] Waterworld\n",
    "* Collect green circles, avoid red ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[119, 100, 115, 97, None]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ple import PLE\n",
    "from ple.games.waterworld import WaterWorld\n",
    "\n",
    "game = WaterWorld() # create our game \n",
    "\n",
    "fps = 30  # fps we want to run at\n",
    "frame_skip = 2\n",
    "num_steps = 2\n",
    "force_fps = False # False for slower speed\n",
    "display_screen = True\n",
    "\n",
    "# make a PLE instance.\n",
    "ple_env = PLE(game, fps=fps, frame_skip=frame_skip, num_steps=num_steps,\n",
    "        force_fps=force_fps, display_screen=display_screen)\n",
    "\n",
    "action_length = len(ple_env.getActionSet())\n",
    "action_list = ple_env.getActionSet()\n",
    "print(action_list)\n",
    "\n",
    "# init agent and game.\n",
    "ple_env.init()\n",
    "#advance game one frame, starts out at a black screen\n",
    "ple_env.act(ple_env.NOOP)\n",
    "\n",
    "for i in range(100):\n",
    "    action = action_list[np.random.randint(0,action_length)]\n",
    "    ple_env.act(action)\n",
    "    if ple_env.game_over():\n",
    "        ple_env.reset_game()\n",
    "        ple_env.act(ple_env.NOOP)\n",
    "    #ple_env.saveScreen(\"test_screen_capture_{}.png\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2.0] MonsterKong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "couldn't import doomish\n",
      "Couldn't import doom\n",
      "[100, 97, 119, 115, 32, None]\n",
      "0\n",
      "reward change 0.1\n",
      "reward change 0.20000000000000004\n",
      "reward change 0.30000000000000004\n",
      "reward change -0.7\n",
      "game over/reset\n",
      "0\n",
      "reward change 0.1\n",
      "reward change -0.9\n",
      "game over/reset\n",
      "0\n",
      "reward change -1.0\n",
      "game over/reset\n",
      "0\n",
      "reward change 0.1\n",
      "reward change 0.2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0005047f0dc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mcurrent_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mple_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcurrent_reward\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mold_reward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reward change {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jim/gym_attempts/PyGame-Learning-Environment/ple/ple.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \"\"\"\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_oneStepAct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_draw_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jim/gym_attempts/PyGame-Learning-Environment/ple/ple.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \"\"\"\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_oneStepAct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_draw_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jim/gym_attempts/PyGame-Learning-Environment/ple/ple.py\u001b[0m in \u001b[0;36m_oneStepAct\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0mtime_elapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_elapsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jim/gym_attempts/PyGame-Learning-Environment/ple/ple.py\u001b[0m in \u001b[0;36m_tick\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;36m1000.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jim/gym_attempts/PyGame-Learning-Environment/ple/games/base/pygamewrapper.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, fps)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mThis\u001b[0m \u001b[0msleeps\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgame\u001b[0m \u001b[0mto\u001b[0m \u001b[0mensure\u001b[0m \u001b[0mit\u001b[0m \u001b[0mruns\u001b[0m \u001b[0mat\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdesired\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick_busy_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madjustRewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ple import PLE\n",
    "from ple.games.monsterkong import MonsterKong\n",
    "\n",
    "game = MonsterKong() # create our game \n",
    "\n",
    "fps = 30  # fps we want to run at\n",
    "frame_skip = 2\n",
    "num_steps = 2\n",
    "force_fps = False # False for slower speed\n",
    "display_screen = True\n",
    "\n",
    "# make a PLE instance.\n",
    "ple_env = PLE(game, fps=fps, frame_skip=frame_skip, num_steps=num_steps,\n",
    "        force_fps=force_fps, display_screen=display_screen)\n",
    "\n",
    "action_length = len(ple_env.getActionSet())\n",
    "action_list = ple_env.getActionSet()\n",
    "print(action_list)\n",
    "\n",
    "# init agent and game.\n",
    "ple_env.init()\n",
    "#advance game one frame, starts out at a black screen\n",
    "ple_env.act(ple_env.NOOP)\n",
    "lives_check = ple_env.lives()\n",
    "print(lives_check)\n",
    "old_reward = 0\n",
    "current_reward = 0\n",
    "for i in range(1000):\n",
    "    action = action_list[np.random.randint(0,action_length)]\n",
    "    current_reward += ple_env.act(action)\n",
    "    if current_reward != old_reward:\n",
    "        print(\"reward change {}\".format(current_reward))\n",
    "        old_reward = current_reward\n",
    "    if ple_env.game_over() or ple_env.lives() != lives_check:\n",
    "        print(\"game over/reset\")\n",
    "        ple_env.reset_game()\n",
    "        print(ple_env.lives())\n",
    "        ple_env.act(ple_env.NOOP)\n",
    "        old_reward = 0\n",
    "        current_reward = 0\n",
    "    #ple_env.saveScreen(\"test_screen_capture_{}.png\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2.1] Flappybird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[119, None]\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ple import PLE\n",
    "from ple.games.flappybird import FlappyBird\n",
    "\n",
    "game = FlappyBird(width=144,height=256)\n",
    "#game = FlappyBird(width=120,height=144) # create our game \n",
    "#game = FlappyBird() # create our game \n",
    "\n",
    "fps = 30  # fps we want to run at\n",
    "frame_skip = 2\n",
    "num_steps = 2\n",
    "force_fps = False # False for slower speed\n",
    "display_screen = True\n",
    "\n",
    "# make a PLE instance.\n",
    "ple_env = PLE(game, fps=fps, frame_skip=frame_skip, num_steps=num_steps,\n",
    "        force_fps=force_fps, display_screen=display_screen)\n",
    "\n",
    "action_length = len(ple_env.getActionSet())\n",
    "action_list = ple_env.getActionSet()\n",
    "print(action_list)\n",
    "print(ple_env.getScreenDims()[0]) #default is 288 by 512, 9x16 ratio\n",
    "# init agent and game.\n",
    "ple_env.init()\n",
    "#advance game one frame, starts out at a black screen\n",
    "ple_env.act(ple_env.NOOP)\n",
    "\n",
    "for i in range(100):\n",
    "    action = action_list[np.random.randint(0,action_length)]\n",
    "    ple_env.act(action)\n",
    "    if ple_env.game_over():\n",
    "        ple_env.reset_game()\n",
    "        ple_env.act(ple_env.NOOP)\n",
    "    #ple_env.saveScreen(\"test_screen_capture_{}.png\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ple_env.game_over()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jim/anaconda3/envs/gym35/bin/python: Error while finding module specification for 'baselines.baselines.deepq.experiments.enjoy_ple_dqn' (ImportError: No module named 'baselines.baselines')\r\n"
     ]
    }
   ],
   "source": [
    "!python -m baselines.deepq.experiments.enjoy_ple_dqn  --env=catcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def random_search_model():\n",
    "#     parser.add_argument('--env', help='environment name', default='catcher')\n",
    "#     parser.add_argument('--seed', help='RNG seed', type=int, default=0)\n",
    "#     parser.add_argument('--prioritized', type=int, default=1)\n",
    "#     parser.add_argument('--dueling', type=int, default=1)\n",
    "#     parser.add_argument('--double_q', type=int, default=1)\n",
    "#     parser.add_argument('--num-timesteps', type=int, default=int(4e6))\n",
    "#     parser.add_argument('--exp_fraction',default=0.1)\n",
    "#     parser.add_argument('--frame_skip', type=int, default=int(2))\n",
    "#     parser.add_argument('--hold_action', type=int, default=int(2))\n",
    "#     parser.add_argument('--learning_rate', type=float, default=float(1e-4))\n",
    "#     parser.add_argument('--buffer_size', type=int, default=int(10000))\n",
    "#     parser.add_argument('--train_freq', type=int, default=int(4))\n",
    "#     parser.add_argument('--target_update', type=int, default=int(1000))\n",
    "#     parser.add_argument('--pr_alpha', type=float, default=float(0.6)) #prioritized replay alpha\n",
    "#     parser.add_argument('--pr_beta', type=float, default=float(0.4))\n",
    "#     parser.add_argument('--pr_eps', type=float, default=float(1e-6))\n",
    "#python -m baselines.deepq.experiments.run_ple_dqn  --env=waterworld --num-timesteps=10000 --prioritized=0 --dueling=0 --double_q=0  --exp_fraction=0.8 --frame_skip=3 --hold_action=3 --learning_rate=0.00008 --buffer_size=15000 --train_freq=3 --target_update=2000\n",
    "    env = 'monsterkong'\n",
    "    num_timesteps = np.random.randint(1000000,4000001)#1000\n",
    "    prioritized = np.random.randint(0,5) #turned into a bool, weighing it more heavily to use this\n",
    "    dueling = np.random.randint(0,5)\n",
    "    double_q = np.random.randint(0,10)\n",
    "    exp_fraction = np.round(np.random.uniform(0.6,0.98),2)\n",
    "    frame_skip = np.random.randint(2,5)\n",
    "    hold_action = frame_skip\n",
    "    learning_rate = 10 **  (-1*np.random.randint(3,6) ) * np.random.randint(1,6)\n",
    "    buffer_size = np.random.randint(10000,50001)\n",
    "    train_freq =  np.random.randint(1,7)\n",
    "    target_update = np.random.randint(1000,10001)\n",
    "\n",
    "    model_string = \"python -m baselines.deepq.experiments.run_ple_dqn --env={} --num-timesteps={} \\\n",
    "    --prioritized={} --dueling={} --double_q={}  --exp_fraction={} --frame_skip={} --hold_action={} \\\n",
    "    --learning_rate={} --buffer_size={} --train_freq={} --target_update={}\".format(\n",
    "        env,num_timesteps,\n",
    "        prioritized,dueling,double_q,exp_fraction,frame_skip,\n",
    "        hold_action,learning_rate,buffer_size,train_freq,target_update)\n",
    "    print(model_string)\n",
    "    ! $model_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m baselines.deepq.experiments.run_ple_dqn --env=catcher --num-timesteps=100     --prioritized=1 --dueling=3 --double_q=2  --exp_fraction=0.88 --frame_skip=3 --hold_action=3     --learning_rate=0.003 --buffer_size=42717 --train_freq=2 --target_update=4854\n",
      "couldn't import doomish\n",
      "Couldn't import doom\n",
      "Logging to /tmp/openai-2017-12-22-14-01-43-673276\n",
      "2017-12-22 14:01:43.778704: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-12-22 14:01:43.778733: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-12-22 14:01:43.778741: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-12-22 14:01:43.778747: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-12-22 14:01:43.778752: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-12-22 14:01:43.888864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2017-12-22 14:01:43.889284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: \n",
      "name: GeForce GTX 1060 6GB\n",
      "major: 6 minor: 1 memoryClockRate (GHz) 1.7715\n",
      "pciBusID 0000:01:00.0\n",
      "Total memory: 5.93GiB\n",
      "Free memory: 4.23GiB\n",
      "2017-12-22 14:01:43.889333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 \n",
      "2017-12-22 14:01:43.889345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y \n",
      "2017-12-22 14:01:43.889355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0)\n",
      "[97, 100, None]\n",
      "WARNING:tensorflow:From /home/jim/gym_attempts/baselines/baselines/deepq/build_graph.py:366: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n",
      "/home/jim/.local/lib/python3.5/site-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/jim/.local/lib/python3.5/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "model saved\n",
      "python -m baselines.deepq.experiments.run_ple_dqn --env=catcher --num-timesteps=100     --prioritized=2 --dueling=1 --double_q=7  --exp_fraction=0.66 --frame_skip=2 --hold_action=2     --learning_rate=3.0000000000000004e-05 --buffer_size=45682 --train_freq=3 --target_update=5173\n",
      "couldn't import doomish\n",
      "Couldn't import doom\n",
      "Logging to /tmp/openai-2017-12-22-14-01-52-866924\n",
      "2017-12-22 14:01:52.944235: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-12-22 14:01:52.944261: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-12-22 14:01:52.944269: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-12-22 14:01:52.944274: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-12-22 14:01:52.944280: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-12-22 14:01:53.068869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2017-12-22 14:01:53.069132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: \n",
      "name: GeForce GTX 1060 6GB\n",
      "major: 6 minor: 1 memoryClockRate (GHz) 1.7715\n",
      "pciBusID 0000:01:00.0\n",
      "Total memory: 5.93GiB\n",
      "Free memory: 4.23GiB\n",
      "2017-12-22 14:01:53.069148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 \n",
      "2017-12-22 14:01:53.069154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y \n",
      "2017-12-22 14:01:53.069161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0)\n",
      "[97, 100, None]\n",
      "WARNING:tensorflow:From /home/jim/gym_attempts/baselines/baselines/deepq/build_graph.py:366: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n",
      "/home/jim/.local/lib/python3.5/site-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/jim/.local/lib/python3.5/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    random_search_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 2, 3, 3, 2, 3, 3, 2, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(2,5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6567447703938406"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform(0.6,0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00030000000000000003"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10 **  (-1*np.random.randint(3,6) ) * np.random.randint(1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10 ** -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
